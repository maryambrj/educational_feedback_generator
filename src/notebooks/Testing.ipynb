{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8692b58",
   "metadata": {},
   "source": [
    "# Testing of the basic grading functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448ee002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from config_manager import ConfigManager, LLMFactory\n",
    "from ai_grading_agent import AIGradingAgent\n",
    "from report_generator import create_combined_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636e378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"HW02/Students\"\n",
    "assignment_id = \"hw2_california_housing\"\n",
    "output_csv = \"HW02/Students/graded_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e937b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMFactory: Creating LLM with provider='anthropic', model='claude-3-5-haiku-latest'\n",
      "✅ Creating Anthropic LLM\n",
      "Using LLM: Anthropic-claude-3-5-haiku-latest\n",
      "Configuration details:\n",
      "  Provider: anthropic\n",
      "  Model: claude-3-5-haiku-latest\n",
      "  Has API key: True\n"
     ]
    }
   ],
   "source": [
    "# Config the LLM\n",
    "config = ConfigManager()\n",
    "llm = LLMFactory.create_llm(config)\n",
    "print(f\"Using LLM: {llm.get_model_name()}\")\n",
    "\n",
    "provider = config.get('llm_settings.provider', 'not_set')\n",
    "model = config.get('llm_settings.model', 'not_set')\n",
    "has_api_key = bool(config.get('llm_settings.api_key', ''))\n",
    "\n",
    "print(f\"Configuration details:\")\n",
    "print(f\"  Provider: {provider}\")\n",
    "print(f\"  Model: {model}\")\n",
    "print(f\"  Has API key: {has_api_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101064fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_grader = AIGradingAgent(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f98651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed content for Silvestri_Luciano.ipynb:\n",
      "{'student_name': 'Silvestri Luciano', 'notebook_path': 'HW02/Students/Silvestri_Luciano.ipynb', 'problems': [{'cell_index': 1, 'content': \"---\\n## Part 1: From Exploration to Engineering (30 points)\\n\\nRead chapter 2 of your textbook. It is a fairly long chapter so I would recommend making an outline to keep track of the many steps. There will be a lecture on this next Monday. You can be pretty sure that the next HW will ask you to make a detailed, step-by-step protocol for a general and complete ML workflow, so you might as well start making that now! \\n\\nStart by documenting the key insights from your in-class exploration. Using your team's findings and discussions:\\n\\n* What did the basic IDA reveal? What would you propose to do about any problems the IDA uncovered?\\n* What did the EDA reveal? What quantitative metrics did you use? What qualitative metrics did you use? What visualizations did you use? \\n* What were the raw features? Explain how your feature engineering decisions connect to specific observations from the in-class activity.\\n\\nNow that you have thought about this and have read chapter 2, write Python to create two visualizations you wish you had made during the ICA and explain why these would have helped with IDA, EDA and feature engineering. \\n\\n\", 'pattern_match': ('1', 'From Exploration to Engineering (30 points)'), 'problem_id': 'part_1'}, {'cell_index': 5, 'content': '--- \\n## Part 2: Train-Test Splits (30 points)\\n\\n(10) In a markdown cell, explain why we need a train-test split in ML. How would you add a validation step to this?\\n\\n(10) Implement a train-test split using `sklearn` with different splits on the CA housing dataset. Check properties of the training portion to see how the split impacted it: use both a visualization (e.g., a pairplot) and some statistics (e.g., `.describe`)\\n\\n(10) In a markdown cell, explain what a \"stratified split\" is and why they can be important. How would that apply to this dataset? \\n', 'pattern_match': ('2', 'Train-Test Splits (30 points)'), 'problem_id': 'part_2'}, {'cell_index': 10, 'content': '---\\n\\n## Part 3: Scikit-Learn API (40 points)\\n\\nThe scikit-learn API is very well designed. It is worth understanding its logic so that you can quickly, easily and correctly get results from it. \\n\\n(10) Within the context of the scikit-learn API, define in a markdown cell what these mean:\\n* estimator,\\n* transformer,\\n* predictor.\\n\\n(10) Within the context of the scikit-learn API, what are the most commonly used _methods_ and _attributes_ that go with these? \\n\\n(10) Create in Python a fake data that consists of a noisy line and fit to it using `LinearRegression` from `.linear_model`. Plot everything together and put the values of the actual weights (slope and intercept) and the predicted values in the title of the plot. Some starter is given below. \\n\\n(10) The scikit-learn API allows for what are called _pipelines_. In a markdown cell, describe what a pipeline is, how to set one up and why these can be extremely important to an ML workflow. ', 'pattern_match': ('3', 'Scikit-Learn API (40 points)'), 'problem_id': 'part_3'}], 'responses': [StudentResponse(problem_id='part_1', part_id=None, content=\"[MARKDOWN CELL]\\n### Basic IDA Findings\\nDuring the initial data assessment (IDA) of the California Housing dataset, the following issues and properties were noted:\\n\\n- **Missing values**: `total_bedrooms` contained NaN entries.\\n- **Feature ranges**: Several numerical variables (e.g., `median_income`, `housing_median_age`) had very different scales, suggesting the need for normalization.\\n- **Potential outliers**: Extremely high values in `median_house_value` and `population` suggested outliers that could affect model training.\\n- **Data types**: All features were numeric except `ocean_proximity`, which is categorical.\\n\\n**Proposed actions:**\\n- Impute missing `total_bedrooms` values using median imputation to avoid bias from skewness.\\n- Apply scaling (e.g., StandardScaler) for features with different ranges.\\n- Investigate and potentially clip or transform extreme outliers.\\n- One-hot encode `ocean_proximity`.\\n\\n### EDA Summary\\n**Quantitative metrics:**\\n- `.describe()` summary statistics for each numerical feature.\\n- Correlation matrix (Pearson’s r) to examine relationships with `median_house_value`.\\n\\n**Qualitative metrics:**\\n- Histograms to examine feature distributions.\\n- Boxplots to check for skewness and outliers.\\n\\n**Visualizations used:**\\n- Scatter plot of `median_income` vs. `median_house_value` (strong positive correlation).\\n- Geographical scatter plot (longitude vs. latitude) colored by house value to reveal spatial patterns.\\n\\n### Feature Engineering Rationale\\n- **`rooms_per_household`**: Derived from `total_rooms / households` — captures housing density.\\n- **`bedrooms_per_room`**: Derived from `total_bedrooms / total_rooms` — proxy for room allocation.\\n- **`population_per_household`**: Derived from `population / households` — measure of crowding.\\n\\nThese features were directly inspired by patterns observed in scatter plots and correlations during EDA.\\n\\n\\n[CODE CELL]\\n# Two Additional Visualizations\\n\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.datasets import fetch_california_housing\\n\\n# Load dataset\\nhousing = fetch_california_housing(as_frame=True).frame\\n\\n# Visualization 1: Pairplot of selected engineered features vs target\\nsns.pairplot(housing[['MedInc', 'AveRooms', 'AveBedrms', 'HouseAge', 'MedHouseVal']],\\n             diag_kind='kde')\\nplt.suptitle('Pairplot of Selected Features and Median House Value', y=1.02)\\nplt.show()\\n\\n# Visualization 2: Correlation heatmap\\ncorr_matrix = housing.corr()\\nplt.figure(figsize=(10, 8))\\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\\nplt.title('Correlation Heatmap')\\nplt.show()\\n\\n[MARKDOWN CELL]\\n**Why these help:**\\n\\t•\\tThe pairplot helps identify linear or nonlinear trends and potential feature-target relationships missed in class.\\n\\t•\\tThe correlation heatmap visually summarizes the strength and direction of feature-target associations, guiding feature selection.\", cell_type='mixed', cell_index=2, execution_output='[OUTPUT from cell 3]\\n[PLAIN TEXT OUTPUT 1]\\n<Figure size 1250x1250 with 30 Axes>\\n\\n[IMAGE OUTPUT 1]\\nFormat: image/png\\nData size: ~616160 characters (base64 encoded)\\nImage contains: Matplotlib plot/visualization\\n\\n\\n[PLAIN TEXT OUTPUT 2]\\n<Figure size 1000x800 with 2 Axes>\\n\\n[IMAGE OUTPUT 2]\\nFormat: image/png\\nData size: ~55896 characters (base64 encoded)\\nImage contains: Matplotlib plot/visualization\\n', has_errors=False), StudentResponse(problem_id='part_2', part_id=None, content=\"[MARKDOWN CELL]\\n### Why Train-Test Split + Validation\\n\\nA train-test split is necessary to estimate generalization performance on unseen data. Training and testing on the same dataset risks overfitting. A validation step (via a hold-out set or cross-validation) enables model selection and hyperparameter tuning without leaking test information.\\n\\n\\n[CODE CELL]\\nfrom sklearn.model_selection import train_test_split\\n\\nX = housing.drop(columns=['MedHouseVal'])\\ny = housing['MedHouseVal']\\n\\n# 80/20 split\\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# 70/30 split\\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\n# Check statistics for training set\\nprint(X_train1.describe())\\n\\n[CODE CELL]\\n# Training portion analysis\\n\\nsns.pairplot(X_train1[['MedInc', 'HouseAge', 'AveRooms']])\\nplt.suptitle('Training Data Feature Relationships', y=1.02)\\nplt.show()\\n\\n[MARKDOWN CELL]\\nInterpretation:\\nThe distribution of features changes slightly with split size — smaller training sets can yield more variance in feature distributions. This can impact model stability, especially with imbalanced features.\\n\\n\\n### Stratified Split Explanation\\n\\nA stratified split ensures the class distribution (for classification) or quantile distribution (for regression) of the target variable is preserved across train and test sets.\\n\\nFor the CA housing dataset, we could stratify by income categories (e.g., bin MedInc into quantiles) to ensure the training and test sets have similar income distribution patterns.\", cell_type='mixed', cell_index=6, execution_output='[OUTPUT from cell 7]\\n[TEXT OUTPUT 1]\\n             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\\\\ncount  16512.000000  16512.000000  16512.000000  16512.000000  16512.000000   \\nmean       3.880754     28.608285      5.435235      1.096685   1426.453004   \\nstd        1.904294     12.602499      2.387375      0.433215   1137.056380   \\nmin        0.499900      1.000000      0.888889      0.333333      3.000000   \\n25%        2.566700     18.000000      4.452055      1.006508    789.000000   \\n50%        3.545800     29.000000      5.235874      1.049286   1167.000000   \\n75%        4.773175     37.000000      6.061037      1.100348   1726.000000   \\nmax       15.000100     52.000000    141.909091     25.636364  35682.000000   \\n\\n           AveOccup      Latitude     Longitude  \\ncount  16512.000000  16512.000000  16512.000000  \\nmean       3.096961     35.643149   -119.582290  \\nstd       11.578744      2.136665      2.005654  \\nmin        0.692308     32.550000   -124.350000  \\n25%        2.428799     33.930000   -121.810000  \\n50%        2.817240     34.260000   -118.510000  \\n75%        3.280000     37.720000   -118.010000  \\nmax     1243.333333     41.950000   -114.310000  \\n\\n\\n[OUTPUT from cell 8]\\n[PLAIN TEXT OUTPUT 1]\\n<Figure size 750x750 with 12 Axes>\\n\\n[IMAGE OUTPUT 1]\\nFormat: image/png\\nData size: ~223284 characters (base64 encoded)\\nImage contains: Matplotlib plot/visualization\\n', has_errors=False), StudentResponse(problem_id='part_3', part_id=None, content='[CODE CELL]\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Set random seed for reproducibility\\nnp.random.seed(42)\\n\\n# Generate x values\\nx = np.linspace(0, 10, 50)\\n\\n# True relationship: y = 2x + 1 with noise\\ntrue_slope = 2\\ntrue_intercept = 1\\nnoise = np.random.normal(0, 1.5, size=len(x))  # Adjust 1.5 to control noise level\\ny = true_slope * x + true_intercept + noise\\n\\n# Create the plot\\nplt.figure(figsize=(10, 6))\\n\\n# Plot noisy data points\\nplt.scatter(x, y, color=\\'blue\\', alpha=0.5, label=\\'Data points\\')\\n\\n# Plot true relationship\\nx_true = np.array([0, 10])\\ny_true = true_slope * x_true + true_intercept\\nplt.plot(x_true, y_true, \\'r--\\', label=\\'True relationship\\')\\n\\nplt.xlabel(\\'x\\')\\nplt.ylabel(\\'y\\')\\nplt.title(\\'Put actual and predicted weights here. Add fit line to the plot.\\')\\nplt.legend()\\nplt.grid(True)\\n\\n\\n\\n[MARKDOWN CELL]\\n**Definitions**\\n\\t•\\tEstimator: Any object with a fit() method that learns from data (e.g., LinearRegression).\\n\\t•\\tTransformer: An estimator with a transform() method that converts input data (e.g., StandardScaler).\\n\\t•\\tPredictor: An estimator with a predict() method that makes predictions from input data.\\n\\n**Common Methods & Attributes**\\n\\t•\\tMethods: fit, transform, fit_transform, predict, score\\n\\t•\\tAttributes: .coef_, .intercept_, .feature_importances_, .n_features_in_\\n\\n[CODE CELL]\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Generate fake data\\nnp.random.seed(42)\\nx = np.linspace(0, 10, 50)\\ntrue_slope = 2\\ntrue_intercept = 1\\nnoise = np.random.normal(0, 1.5, size=len(x))\\ny = true_slope * x + true_intercept + noise\\n\\n# Fit model\\nX = x.reshape(-1, 1)\\nmodel = LinearRegression()\\nmodel.fit(X, y)\\npred_y = model.predict(X)\\n\\n# Plot\\nplt.figure(figsize=(10, 6))\\nplt.scatter(x, y, color=\\'blue\\', alpha=0.5, label=\\'Data points\\')\\nplt.plot(x, true_slope*x + true_intercept, \\'r--\\', label=\\'True relationship\\')\\nplt.plot(x, pred_y, \\'g-\\', label=\\'Predicted line\\')\\nplt.xlabel(\\'x\\')\\nplt.ylabel(\\'y\\')\\nplt.title(f\"True slope={true_slope}, intercept={true_intercept} | \"\\n          f\"Pred slope={model.coef_[0]:.2f}, intercept={model.intercept_:.2f}\")\\nplt.legend()\\nplt.grid(True)\\nplt.show()\\n\\n[MARKDOWN CELL]\\n**Interpretation:**\\nThe predicted slope and intercept should be close to the true values, with small differences due to noise.\\n\\n### Pipelines\\n\\nA pipeline chains preprocessing steps and a final estimator into one object. This ensures transformations are applied consistently during training and inference, and helps avoid data leakage.\\n\\n\\n**Benefits:**\\n\\t•\\tReproducibility\\n\\t•\\tCleaner code\\n\\t•\\tPrevents applying preprocessing fitted on the test set\\n\\n\\n[CODE CELL]\\n# Example setup\\n# \\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LinearRegression\\n\\npipeline = Pipeline([\\n    (\\'scaler\\', StandardScaler()),\\n    (\\'linreg\\', LinearRegression())\\n])\\n\\npipeline.fit(X_train1, y_train1)\\npreds = pipeline.predict(X_test1)', cell_type='mixed', cell_index=11, execution_output='[OUTPUT from cell 11]\\n[PLAIN TEXT OUTPUT 1]\\n<Figure size 1000x600 with 1 Axes>\\n\\n[IMAGE OUTPUT 1]\\nFormat: image/png\\nData size: ~48228 characters (base64 encoded)\\nImage contains: Matplotlib plot/visualization\\n\\n\\n[OUTPUT from cell 13]\\n[PLAIN TEXT OUTPUT 1]\\n<Figure size 1000x600 with 1 Axes>\\n\\n[IMAGE OUTPUT 1]\\nFormat: image/png\\nData size: ~61848 characters (base64 encoded)\\nImage contains: Matplotlib plot/visualization\\n', has_errors=False)], 'total_cells': 17}\n",
      "Response for part_1:\n",
      "StudentResponse(problem_id='part_1', part_id=None, content=\"[MARKDOWN CELL]\\n### Basic IDA Findings\\nDuring the initial data assessment (IDA) of the California Housing dataset, the following issues and properties were noted:\\n\\n- **Missing values**: `total_bedrooms` contained NaN entries.\\n- **Feature ranges**: Several numerical variables (e.g., `median_income`, `housing_median_age`) had very different scales, suggesting the need for normalization.\\n- **Potential outliers**: Extremely high values in `median_house_value` and `population` suggested outliers that could affect model training.\\n- **Data types**: All features were numeric except `ocean_proximity`, which is categorical.\\n\\n**Proposed actions:**\\n- Impute missing `total_bedrooms` values using median imputation to avoid bias from skewness.\\n- Apply scaling (e.g., StandardScaler) for features with different ranges.\\n- Investigate and potentially clip or transform extreme outliers.\\n- One-hot encode `ocean_proximity`.\\n\\n### EDA Summary\\n**Quantitative metrics:**\\n- `.describe()` summary statistics for each numerical feature.\\n- Correlation matrix (Pearson’s r) to examine relationships with `median_house_value`.\\n\\n**Qualitative metrics:**\\n- Histograms to examine feature distributions.\\n- Boxplots to check for skewness and outliers.\\n\\n**Visualizations used:**\\n- Scatter plot of `median_income` vs. `median_house_value` (strong positive correlation).\\n- Geographical scatter plot (longitude vs. latitude) colored by house value to reveal spatial patterns.\\n\\n### Feature Engineering Rationale\\n- **`rooms_per_household`**: Derived from `total_rooms / households` — captures housing density.\\n- **`bedrooms_per_room`**: Derived from `total_bedrooms / total_rooms` — proxy for room allocation.\\n- **`population_per_household`**: Derived from `population / households` — measure of crowding.\\n\\nThese features were directly inspired by patterns observed in scatter plots and correlations during EDA.\\n\\n\\n[CODE CELL]\\n# Two Additional Visualizations\\n\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.datasets import fetch_california_housing\\n\\n# Load dataset\\nhousing = fetch_california_housing(as_frame=True).frame\\n\\n# Visualization 1: Pairplot of selected engineered features vs target\\nsns.pairplot(housing[['MedInc', 'AveRooms', 'AveBedrms', 'HouseAge', 'MedHouseVal']],\\n             diag_kind='kde')\\nplt.suptitle('Pairplot of Selected Features and Median House Value', y=1.02)\\nplt.show()\\n\\n# Visualization 2: Correlation heatmap\\ncorr_matrix = housing.corr()\\nplt.figure(figsize=(10, 8))\\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\\nplt.title('Correlation Heatmap')\\nplt.show()\\n\\n[MARKDOWN CELL]\\n**Why these help:**\\n\\t•\\tThe pairplot helps identify linear or nonlinear trends and potential feature-target relationships missed in class.\\n\\t•\\tThe correlation heatmap visually summarizes the strength and direction of feature-target associations, guiding feature selection.\", cell_type='mixed', cell_index=2, execution_output='[OUTPUT from cell 3]\\n[PLAIN TEXT OUTPUT 1]\\n<Figure size 1250x1250 with 30 Axes>\\n\\n[IMAGE OUTPUT 1]\\nFormat: image/png\\nData size: ~616160 characters (base64 encoded)\\nImage contains: Matplotlib plot/visualization\\n\\n\\n[PLAIN TEXT OUTPUT 2]\\n<Figure size 1000x800 with 2 Axes>\\n\\n[IMAGE OUTPUT 2]\\nFormat: image/png\\nData size: ~55896 characters (base64 encoded)\\nImage contains: Matplotlib plot/visualization\\n', has_errors=False)\n",
      "Response for part_2:\n",
      "StudentResponse(problem_id='part_2', part_id=None, content=\"[MARKDOWN CELL]\\n### Why Train-Test Split + Validation\\n\\nA train-test split is necessary to estimate generalization performance on unseen data. Training and testing on the same dataset risks overfitting. A validation step (via a hold-out set or cross-validation) enables model selection and hyperparameter tuning without leaking test information.\\n\\n\\n[CODE CELL]\\nfrom sklearn.model_selection import train_test_split\\n\\nX = housing.drop(columns=['MedHouseVal'])\\ny = housing['MedHouseVal']\\n\\n# 80/20 split\\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# 70/30 split\\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\n# Check statistics for training set\\nprint(X_train1.describe())\\n\\n[CODE CELL]\\n# Training portion analysis\\n\\nsns.pairplot(X_train1[['MedInc', 'HouseAge', 'AveRooms']])\\nplt.suptitle('Training Data Feature Relationships', y=1.02)\\nplt.show()\\n\\n[MARKDOWN CELL]\\nInterpretation:\\nThe distribution of features changes slightly with split size — smaller training sets can yield more variance in feature distributions. This can impact model stability, especially with imbalanced features.\\n\\n\\n### Stratified Split Explanation\\n\\nA stratified split ensures the class distribution (for classification) or quantile distribution (for regression) of the target variable is preserved across train and test sets.\\n\\nFor the CA housing dataset, we could stratify by income categories (e.g., bin MedInc into quantiles) to ensure the training and test sets have similar income distribution patterns.\", cell_type='mixed', cell_index=6, execution_output='[OUTPUT from cell 7]\\n[TEXT OUTPUT 1]\\n             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\\\\ncount  16512.000000  16512.000000  16512.000000  16512.000000  16512.000000   \\nmean       3.880754     28.608285      5.435235      1.096685   1426.453004   \\nstd        1.904294     12.602499      2.387375      0.433215   1137.056380   \\nmin        0.499900      1.000000      0.888889      0.333333      3.000000   \\n25%        2.566700     18.000000      4.452055      1.006508    789.000000   \\n50%        3.545800     29.000000      5.235874      1.049286   1167.000000   \\n75%        4.773175     37.000000      6.061037      1.100348   1726.000000   \\nmax       15.000100     52.000000    141.909091     25.636364  35682.000000   \\n\\n           AveOccup      Latitude     Longitude  \\ncount  16512.000000  16512.000000  16512.000000  \\nmean       3.096961     35.643149   -119.582290  \\nstd       11.578744      2.136665      2.005654  \\nmin        0.692308     32.550000   -124.350000  \\n25%        2.428799     33.930000   -121.810000  \\n50%        2.817240     34.260000   -118.510000  \\n75%        3.280000     37.720000   -118.010000  \\nmax     1243.333333     41.950000   -114.310000  \\n\\n\\n[OUTPUT from cell 8]\\n[PLAIN TEXT OUTPUT 1]\\n<Figure size 750x750 with 12 Axes>\\n\\n[IMAGE OUTPUT 1]\\nFormat: image/png\\nData size: ~223284 characters (base64 encoded)\\nImage contains: Matplotlib plot/visualization\\n', has_errors=False)\n",
      "Response for part_3:\n",
      "StudentResponse(problem_id='part_3', part_id=None, content='[CODE CELL]\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Set random seed for reproducibility\\nnp.random.seed(42)\\n\\n# Generate x values\\nx = np.linspace(0, 10, 50)\\n\\n# True relationship: y = 2x + 1 with noise\\ntrue_slope = 2\\ntrue_intercept = 1\\nnoise = np.random.normal(0, 1.5, size=len(x))  # Adjust 1.5 to control noise level\\ny = true_slope * x + true_intercept + noise\\n\\n# Create the plot\\nplt.figure(figsize=(10, 6))\\n\\n# Plot noisy data points\\nplt.scatter(x, y, color=\\'blue\\', alpha=0.5, label=\\'Data points\\')\\n\\n# Plot true relationship\\nx_true = np.array([0, 10])\\ny_true = true_slope * x_true + true_intercept\\nplt.plot(x_true, y_true, \\'r--\\', label=\\'True relationship\\')\\n\\nplt.xlabel(\\'x\\')\\nplt.ylabel(\\'y\\')\\nplt.title(\\'Put actual and predicted weights here. Add fit line to the plot.\\')\\nplt.legend()\\nplt.grid(True)\\n\\n\\n\\n[MARKDOWN CELL]\\n**Definitions**\\n\\t•\\tEstimator: Any object with a fit() method that learns from data (e.g., LinearRegression).\\n\\t•\\tTransformer: An estimator with a transform() method that converts input data (e.g., StandardScaler).\\n\\t•\\tPredictor: An estimator with a predict() method that makes predictions from input data.\\n\\n**Common Methods & Attributes**\\n\\t•\\tMethods: fit, transform, fit_transform, predict, score\\n\\t•\\tAttributes: .coef_, .intercept_, .feature_importances_, .n_features_in_\\n\\n[CODE CELL]\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Generate fake data\\nnp.random.seed(42)\\nx = np.linspace(0, 10, 50)\\ntrue_slope = 2\\ntrue_intercept = 1\\nnoise = np.random.normal(0, 1.5, size=len(x))\\ny = true_slope * x + true_intercept + noise\\n\\n# Fit model\\nX = x.reshape(-1, 1)\\nmodel = LinearRegression()\\nmodel.fit(X, y)\\npred_y = model.predict(X)\\n\\n# Plot\\nplt.figure(figsize=(10, 6))\\nplt.scatter(x, y, color=\\'blue\\', alpha=0.5, label=\\'Data points\\')\\nplt.plot(x, true_slope*x + true_intercept, \\'r--\\', label=\\'True relationship\\')\\nplt.plot(x, pred_y, \\'g-\\', label=\\'Predicted line\\')\\nplt.xlabel(\\'x\\')\\nplt.ylabel(\\'y\\')\\nplt.title(f\"True slope={true_slope}, intercept={true_intercept} | \"\\n          f\"Pred slope={model.coef_[0]:.2f}, intercept={model.intercept_:.2f}\")\\nplt.legend()\\nplt.grid(True)\\nplt.show()\\n\\n[MARKDOWN CELL]\\n**Interpretation:**\\nThe predicted slope and intercept should be close to the true values, with small differences due to noise.\\n\\n### Pipelines\\n\\nA pipeline chains preprocessing steps and a final estimator into one object. This ensures transformations are applied consistently during training and inference, and helps avoid data leakage.\\n\\n\\n**Benefits:**\\n\\t•\\tReproducibility\\n\\t•\\tCleaner code\\n\\t•\\tPrevents applying preprocessing fitted on the test set\\n\\n\\n[CODE CELL]\\n# Example setup\\n# \\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LinearRegression\\n\\npipeline = Pipeline([\\n    (\\'scaler\\', StandardScaler()),\\n    (\\'linreg\\', LinearRegression())\\n])\\n\\npipeline.fit(X_train1, y_train1)\\npreds = pipeline.predict(X_test1)', cell_type='mixed', cell_index=11, execution_output='[OUTPUT from cell 11]\\n[PLAIN TEXT OUTPUT 1]\\n<Figure size 1000x600 with 1 Axes>\\n\\n[IMAGE OUTPUT 1]\\nFormat: image/png\\nData size: ~48228 characters (base64 encoded)\\nImage contains: Matplotlib plot/visualization\\n\\n\\n[OUTPUT from cell 13]\\n[PLAIN TEXT OUTPUT 1]\\n<Figure size 1000x600 with 1 Axes>\\n\\n[IMAGE OUTPUT 1]\\nFormat: image/png\\nData size: ~61848 characters (base64 encoded)\\nImage contains: Matplotlib plot/visualization\\n', has_errors=False)\n"
     ]
    }
   ],
   "source": [
    "# Check the parsed content\n",
    "notebook_files = [f for f in os.listdir(directory_path) if f.endswith('.ipynb')]\n",
    "\n",
    "for notebook_file in notebook_files:\n",
    "    notebook_path = os.path.join(directory_path, notebook_file)\n",
    "    parsed_content = ai_grader.parser.parse_notebook(notebook_path)\n",
    "    print(f\"Parsed content for {notebook_file}:\")\n",
    "    print(parsed_content)\n",
    "\n",
    "# Print responses\n",
    "for response in parsed_content['responses']:\n",
    "    print(f\"Response for {response.problem_id}:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee328e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rubric for hw2_california_housing:\n",
      "{'part_1': ProblemRubric(problem_id='part_1', total_points=30, criteria=[GradingCriterion(name='basic_ida', max_points=5, description='Basic IDA findings and proposed solutions', guidelines='5 pts: Clearly describes key findings, identifies issues (missing data, anomalies), and proposes justified fixes\\n3-4 pts: Identifies some findings/issues but limited or unclear proposed solutions\\n1-2 pts: Minimal or incomplete description, vague fixes\\nLook for specific observations about data structure, missing values, data types, and concrete solutions\\n'), GradingCriterion(name='eda_summary', max_points=5, description='EDA summary with quantitative/qualitative metrics and visualizations', guidelines='5 pts: Clear explanation of quantitative metrics, qualitative observations, and visualizations used\\n3-4 pts: Some metrics or visualizations missing or inadequately explained\\n1-2 pts: Very superficial coverage\\nAssess coverage of statistical analysis, pattern identification, and visualization quality\\n'), GradingCriterion(name='feature_engineering_rationale', max_points=5, description='Feature engineering decisions connected to class observations', guidelines='5 pts: Features linked explicitly to in-class observations; rationale is data-driven\\n3-4 pts: Some connection to observations but incomplete reasoning\\n1-2 pts: No clear linkage between features and data insights\\nLook for explicit connections between EDA findings and feature choices\\n'), GradingCriterion(name='visualization_code', max_points=5, description='Code correctness for two new visualizations', guidelines='5 pts: Runs without error, uses appropriate Python plotting libraries, plots are clear and labeled\\n3-4 pts: Minor errors in code or plot formatting\\n1-2 pts: Significant errors or poorly formatted plots\\nCheck for working code, proper imports, clear labels, titles, and legends\\n'), GradingCriterion(name='visualization_insight', max_points=5, description='Insights from visualizations tied to analysis decisions', guidelines='5 pts: Explanation clearly ties visualizations to IDA/EDA/feature engineering decisions\\n3-4 pts: Some connection but incomplete reasoning\\n1-2 pts: Visualizations not well connected to analysis\\nEvaluate how well the plots support the overall analysis narrative\\n'), GradingCriterion(name='visualization_originality', max_points=5, description='Originality and value of chosen visualizations', guidelines=\"5 pts: Chosen plots add meaningful new perspective not covered in ICA\\n3-4 pts: Some new insights but limited added value\\n1-2 pts: Plots are repetitive or don't add new information\\nLook for creative approaches that reveal new patterns or relationships\\n\")], problem_statement='From Exploration to Engineering - Transform collaborative insights from class into concrete feature engineering decisions', expected_response_type='mixed', context='Students should build on team findings from in-class exploration to document IDA/EDA insights and create two new visualizations that support their analysis'), 'part_2': ProblemRubric(problem_id='part_2', total_points=30, criteria=[GradingCriterion(name='train_test_concept', max_points=5, description='Understanding of why train-test splits are needed and validation role', guidelines='5 pts: Correct reasoning about generalization, overfitting, and validation role\\n3-4 pts: Partial explanation or missing a key concept\\n1-2 pts: Misunderstands or omits critical points\\nLook for understanding of overfitting, generalization, model evaluation, and the role of validation sets\\n'), GradingCriterion(name='stratified_split_concept', max_points=5, description='Understanding of stratified splits and application to dataset', guidelines='5 pts: Clear definition, relevance to dataset, and correct application reasoning\\n3-4 pts: Mostly correct but missing context for this dataset\\n1-2 pts: Vague or incorrect explanation\\nEvaluate understanding of when and why to use stratified splits, especially for this housing dataset\\n'), GradingCriterion(name='implementation_correctness', max_points=10, description='Correct implementation of train_test_split with multiple ratios', guidelines='5 pts: Correct train_test_split usage with multiple split ratios, reproducible (random_state), appropriate imports\\n3-4 pts: Minor errors or limited exploration of different splits\\n1-2 pts: Incorrect method or missing required variations\\nCheck for proper sklearn usage, random_state for reproducibility, and exploration of different split ratios\\n'), GradingCriterion(name='analysis_code', max_points=5, description='Code quality for training portion analysis', guidelines='5 pts: Correct generation of statistics (.describe) and visualization (pairplot or similar)\\n3-4 pts: Minor issues in statistical analysis or visualization\\n1-2 pts: Significant errors in analysis code\\nLook for proper use of .describe(), appropriate visualizations like pairplots, and clean code\\n'), GradingCriterion(name='analysis_interpretation', max_points=5, description='Interpretation of how splits affect data distribution', guidelines='5 pts: Explains how different splits affect distribution, notes patterns/anomalies\\n3-4 pts: Some interpretation but missing key observations\\n1-2 pts: Minimal or incorrect interpretation\\nAssess understanding of how split ratios impact data distribution and model training\\n')], problem_statement='Train-Test Splits - Understanding and implementation of data splitting strategies', expected_response_type='mixed', context='Students should demonstrate understanding of train-test splits, validation, and stratification concepts with proper implementation'), 'part_3': ProblemRubric(problem_id='part_3', total_points=40, criteria=[GradingCriterion(name='api_definitions', max_points=5, description='Accurate definitions of estimator, transformer, predictor in sklearn context', guidelines='5 pts: Accurate, scikit-learn specific definitions\\n3-4 pts: Mostly correct but imprecise wording\\n1-2 pts: Generic or incorrect definitions\\nLook for sklearn-specific understanding, not just generic ML definitions\\n'), GradingCriterion(name='methods_attributes', max_points=5, description='Knowledge of common sklearn methods and attributes', guidelines='5 pts: Correctly lists relevant ones (fit, transform, predict, score, .coef_, .intercept_)\\n3-4 pts: Missing some key methods/attributes\\n1-2 pts: Mostly incorrect or incomplete\\nCheck for understanding of core sklearn interface methods and common model attributes\\n'), GradingCriterion(name='pipeline_concept', max_points=10, description='Understanding of ML pipelines - concept, setup, and importance', guidelines='10 pts: Explains concept, setup steps, and why they are important to ML workflows\\n6-9 pts: Partial coverage of pipeline concepts\\n1-5 pts: Minimal or inaccurate pipeline explanation\\nEvaluate understanding of pipeline benefits, how to create them, and their role in ML workflows\\n'), GradingCriterion(name='explanation_quality', max_points=10, description='Overall clarity and technical depth of explanations', guidelines='10 pts: Precise, professional, and context-aware writing with appropriate technical depth\\n6-9 pts: Generally clear but may lack precision or depth in some areas\\n1-5 pts: Unclear, imprecise, or superficial explanations\\nAssess overall communication quality, technical accuracy, and depth of understanding\\n'), GradingCriterion(name='regression_code', max_points=5, description='Code correctness for fake data generation and linear regression', guidelines='5 pts: Generates noisy linear data, fits model, plots true vs predicted line, includes slope/intercept in title\\n3-4 pts: Minor issues in implementation or plotting\\n1-2 pts: Significant errors in code or missing required elements\\nCheck for proper data generation, model fitting, visualization, and parameter display\\n'), GradingCriterion(name='regression_interpretation', max_points=5, description='Interpretation of linear regression results and parameter differences', guidelines='5 pts: Explains differences between true and predicted parameters, mentions potential reasons for discrepancies\\n3-4 pts: Some interpretation but missing key insights\\n1-2 pts: Minimal or incorrect interpretation\\nLook for understanding of why fitted parameters differ from true parameters (noise, sample size, etc.)\\n')], problem_statement='Scikit-Learn API - Understanding core ML workflow concepts and implementation', expected_response_type='mixed', context='Students should demonstrate deep understanding of sklearn API concepts and implement a complete linear regression example')}\n"
     ]
    }
   ],
   "source": [
    "# Check assignment rubric\n",
    "assignment_rubric = ai_grader.rubric_manager.load_assignment_rubric(assignment_id)\n",
    "print(f\"Loaded rubric for {assignment_id}:\")\n",
    "print(assignment_rubric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a39b4953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Graded part_1: 29/30 (96.7%)\n",
      "  Graded part_2: 27/30 (90.0%)\n",
      "  Graded part_3: 36/40 (90.0%)\n"
     ]
    }
   ],
   "source": [
    "# Grade each response\n",
    "notebook_results = []\n",
    "\n",
    "for response in parsed_content['responses']:\n",
    "    if response.problem_id in assignment_rubric:\n",
    "        rubric = assignment_rubric[response.problem_id]\n",
    "        \n",
    "        # Add assignment context\n",
    "        context = f\"Assignment: {assignment_id}\"\n",
    "        \n",
    "        result = ai_grader.grader.grade_response(response, rubric, context)\n",
    "        result.student_name = \"Silvestri, Luciano\"\n",
    "        \n",
    "        notebook_results.append(result)\n",
    "        print(f\"  Graded {response.problem_id}: {result.total_score}/{result.max_possible} ({result.percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"  Warning: No rubric found for {response.problem_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9474cd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exceptional work demonstrating comprehensive understanding of data analysis and feature engineering for the California Housing dataset. The student provided a thorough, well-structured response that effectively integrates technical implementation with clear, data-driven reasoning.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_results[0].feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583583d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmse492",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
